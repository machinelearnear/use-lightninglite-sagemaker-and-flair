{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22af41dc",
   "metadata": {},
   "source": [
    "# Distributed Training with LightningLite, SageMaker, and Flair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402171a0",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "- [Getting Started with Tensor Parallelism using the SageMaker Model Parallelism Library\n",
    "](https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt-j/11_train_gptj_smp_tensor_parallel_notebook.ipynb)\n",
    "- [LightningLite Integration with Flair](https://github.com/flairNLP/flair/pull/2700)\n",
    "- [LIGHTNINGLITE - STEPPING STONE TO LIGHTNING](https://pytorch-lightning.readthedocs.io/en/stable/starter/lightning_lite.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be26cc14",
   "metadata": {},
   "source": [
    "## Debug training\n",
    "- [PYTORCH_LIGHTNING.LITE.LIGHTNINGLITE](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.lite.LightningLite.html#pytorch_lightning.lite.LightningLite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "import flair\n",
    "from flair import set_seed\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from custom_trainer import LiteTrainer # changed\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, Union, cast # changed\n",
    "\n",
    "logger = logging.getLogger(\"flair\")\n",
    "logger.setLevel(level=\"INFO\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class model_args:\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"The model checkpoint for weights initialization.\"},\n",
    "    )\n",
    "    layers: str = field(default=\"-1\", metadata={\"help\": \"Layers to be fine-tuned.\"})\n",
    "    subtoken_pooling: str = field(\n",
    "        default=\"first\",\n",
    "        metadata={\"help\": \"Subtoken pooling strategy used for fine-tuned.\"},\n",
    "    )\n",
    "    hidden_size: int = field(default=256, metadata={\"help\": \"Hidden size for NER model.\"})\n",
    "    use_crf: bool = field(default=False, metadata={\"help\": \"Whether to use a CRF on-top or not.\"})\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class training_args:\n",
    "    num_epochs: int = field(default=10, metadata={\"help\": \"The number of training epochs.\"})\n",
    "    batch_size: int = field(default=8, metadata={\"help\": \"Batch size used for training.\"})\n",
    "    mini_batch_chunk_size: int = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"If smaller than batch size, batches will be chunked.\"},\n",
    "    )\n",
    "    learning_rate: float = field(default=5e-05, metadata={\"help\": \"Learning rate\"})\n",
    "    seed: int = field(default=42, metadata={\"help\": \"Seed used for reproducible fine-tuning results.\"})\n",
    "    device: str = field(default=\"cuda:0\", metadata={\"help\": \"CUDA device string.\"})\n",
    "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for optimizer.\"})\n",
    "    embeddings_storage_mode: str = field(default=\"none\", metadata={\"help\": \"Defines embedding storage method.\"})\n",
    "    accelerator: Optional[str] = field(default=None, metadata={\"help\": \"Choose the hardware to run on e.g. 'gpu'.\"})\n",
    "    strategy: Optional[str] = field(\n",
    "        default=None, \n",
    "        metadata={\"help\": \"Strategy for how to run across multiple devices e.g. 'ddp', 'deepspeed'.\"})\n",
    "    devices: Optional[int] = field(\n",
    "        default=None, \n",
    "        metadata={\"help\": \"Number of devices to train on (int), which GPUs to train on (list or str)\"})\n",
    "    num_nodes: Optional[int] = field(default=1, metadata={\"help\": \"Number of GPU nodes for distributed training.\"})\n",
    "    precision: Optional[int] = field(default=32, metadata={\"help\": \"Choose training precision to use.\"})\n",
    "\n",
    "@dataclass\n",
    "class flert_args:\n",
    "    context_size: int = field(default=0, metadata={\"help\": \"Context size when using FLERT approach.\"})\n",
    "    respect_document_boundaries: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to respect document boundaries or not when using FLERT.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class data_args:\n",
    "    dataset_name: str = field(metadata={\"help\": \"Flair NER dataset name.\"})\n",
    "    dataset_arguments: str = field(default=\"\", metadata={\"help\": \"Dataset arguments for Flair NER dataset.\"})\n",
    "    output_dir: str = field(\n",
    "        default=\"resources/taggers/ner\",\n",
    "        metadata={\"help\": \"Defines output directory for final fine-tuned model.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def get_flair_corpus(data_args):\n",
    "    ner_task_mapping = {}\n",
    "\n",
    "    for name, obj in inspect.getmembers(flair.datasets.sequence_labeling):\n",
    "        if inspect.isclass(obj):\n",
    "            if name.startswith(\"NER\") or name.startswith(\"CONLL\") or name.startswith(\"WNUT\"):\n",
    "                ner_task_mapping[name] = obj\n",
    "\n",
    "    dataset_args = {}\n",
    "    dataset_name = data_args.dataset_name\n",
    "\n",
    "    if data_args.dataset_arguments:\n",
    "        dataset_args = json.loads(data_args.dataset_arguments)\n",
    "\n",
    "    if dataset_name not in ner_task_mapping:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} is not a valid Flair datasets name!\")\n",
    "\n",
    "    return ner_task_mapping[dataset_name](**dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.dataset_name = 'NER_ENGLISH_PERSON'\n",
    "data_args.output_dir = 'ner-english-test'\n",
    "model_args.model_name_or_path = 'xlm-roberta-base'\n",
    "training_args.batch_size = 32\n",
    "training_args.learning_rate = 5e-05\n",
    "training_args.accelerator = 'gpu'\n",
    "training_args.strategy = None \n",
    "training_args.devices = 1\n",
    "training_args.num_nodes = 1\n",
    "training_args.precision = 16\n",
    "training_args.num_epochs = 50\n",
    "training_args.context_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf0998",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)\n",
    "\n",
    "flair.device = training_args.device\n",
    "\n",
    "corpus = get_flair_corpus(data_args)\n",
    "\n",
    "logger.info(corpus)\n",
    "\n",
    "tag_type: str = \"ner\"\n",
    "tag_dictionary = corpus.make_label_dictionary(tag_type)\n",
    "logger.info(tag_dictionary)\n",
    "\n",
    "embeddings = TransformerWordEmbeddings(\n",
    "    model=model_args.model_name_or_path,\n",
    "    layers=model_args.layers,\n",
    "    subtoken_pooling=model_args.subtoken_pooling,\n",
    "    fine_tune=True,\n",
    "    use_context=flert_args.context_size,\n",
    "    respect_document_boundaries=flert_args.respect_document_boundaries,\n",
    ")\n",
    "\n",
    "tagger = SequenceTagger(\n",
    "    hidden_size=model_args.hidden_size,\n",
    "    embeddings=embeddings,\n",
    "    tag_dictionary=tag_dictionary,\n",
    "    tag_type=tag_type,\n",
    "    use_crf=model_args.use_crf,\n",
    "    use_rnn=False,\n",
    "    reproject_embeddings=False,\n",
    ")\n",
    "\n",
    "# changed\n",
    "trainer = LiteTrainer( \n",
    "    accelerator=training_args.accelerator,\n",
    "    strategy=training_args.strategy,\n",
    "    devices=training_args.devices,\n",
    "    num_nodes=training_args.num_nodes,\n",
    "    precision=training_args.precision,\n",
    ")\n",
    "\n",
    "# changed\n",
    "trainer.train(tagger, corpus,\n",
    "    data_args.output_dir,\n",
    "    learning_rate=training_args.learning_rate,\n",
    "    mini_batch_size=training_args.batch_size,\n",
    "    mini_batch_chunk_size=training_args.mini_batch_chunk_size,\n",
    "    max_epochs=training_args.num_epochs,\n",
    "    embeddings_storage_mode=training_args.embeddings_storage_mode,\n",
    "    weight_decay=training_args.weight_decay,\n",
    ")\n",
    "\n",
    "torch.save(model_args, os.path.join(data_args.output_dir, \"model_args.bin\"))\n",
    "torch.save(training_args, os.path.join(data_args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "# finally, print model card for information\n",
    "tagger.print_model_card()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d4f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_machinelearnear-lightninglite-sagemaker-flair",
   "language": "python",
   "name": "conda_machinelearnear-lightninglite-sagemaker-flair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
