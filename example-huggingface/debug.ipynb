{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Distributed Training Demo\n",
    "### Distributed Token Classification (NER) with `transformers` scripts +  `Trainer` and `conll2003` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source: https://github.com/huggingface/transformers/tree/v4.17.0/examples/pytorch/token-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions) \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end `distributed` \"Token Classification\" (NER) example. In this demo, we will use the Hugging Face `transformers` together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer for token-classification on multiple-gpus. In particular, the pre-trained model will be fine-tuned using the `conll2003` dataset. The demo will use the new `smdistributed` library to run training on multiple gpus as training scripting we are going to use one of the `transformers` [example scripts from the repository](https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_qa.py).\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\"  --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::110564771975:role/service-role/AmazonSageMaker-ExecutionRole-20210806T162946\n",
      "sagemaker bucket: sagemaker-eu-west-2-110564771975\n",
      "sagemaker session region: eu-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "```python\n",
    "hyperparameters = {\n",
    "\t'model_name_or_path':'philschmid/distilroberta-base-ner-conll2003',\n",
    "\t'output_dir':'/opt/ml/model',\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 32,\n",
    "\t# add your remaining hyperparameters\n",
    "\t# more info here https://github.com/huggingface/transformers/tree/v4.17.0/examples/pytorch/token-classification\n",
    "}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_ner.py',\n",
    "    source_dir='./examples/pytorch/token-classification',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    git_config=git_config,\n",
    "    transformers_version='4.17.0',\n",
    "    pytorch_version='1.10.2',\n",
    "    py_version='py38',\n",
    "    hyperparameters = hyperparameters\n",
    ")\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name philschmid/distilroberta-base-ner-conll2003 --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data from Flair, split into `train`/`dev`/`test`, and upload to S3\n",
    "- https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md\n",
    "- https://github.com/huggingface/transformers/blob/main/tests/fixtures/tests_samples/conll/sample.json\n",
    "- https://github.com/huggingface/transformers/issues/8698\n",
    "- https://huggingface.co/docs/datasets/loading#json-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "import inspect\n",
    "import json\n",
    "from os.path import exists as path_exists\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of what `train.json` should look like\n",
    "\n",
    "```python\n",
    "{\"id\": \"1\", \"tokens\": [\"APPLICATION\", \"and\", \"Affidavit\", \"for\", \"Search\", ...], \"ner_tags\": [\"O\", \"O\", \"O\", \"O\", \"O\", ...]}\n",
    "\n",
    "...\n",
    "\n",
    "{\"id\": \"n\", \"tokens\": [\"APPLICATION\", \"and\", \"Affidavit\", \"for\", \"Search\", ...], \"ner_tags\": [\"O\", \"O\", \"O\", \"O\", \"O\", ...]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flair_corpus(dataset_name,dataset_arguments:str=\"\"):\n",
    "    ner_task_mapping = {}\n",
    "\n",
    "    for name, obj in inspect.getmembers(flair.datasets.sequence_labeling):\n",
    "        if inspect.isclass(obj):\n",
    "            if name.startswith(\"NER\") or name.startswith(\"CONLL\") or name.startswith(\"WNUT\"):\n",
    "                ner_task_mapping[name] = obj\n",
    "\n",
    "    dataset_args = {}\n",
    "\n",
    "    if dataset_arguments:\n",
    "        dataset_args = json.loads(dataset_arguments)\n",
    "\n",
    "    if dataset_name not in ner_task_mapping:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} is not a valid Flair datasets name!\")\n",
    "\n",
    "    return ner_task_mapping[dataset_name](**dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_flair_corpus_to_s3(corpus, filepath, key_prefix = 'distributed_conll2003_data'):\n",
    "    if not path_exists('dataset'): !mkdir dataset\n",
    "    path = f'dataset/{filepath}'\n",
    "    \n",
    "    lines = []\n",
    "    for idx, sentence in enumerate(corpus):\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        for token in sentence:\n",
    "            try: tag = token.tag\n",
    "            except: tag = \"O\"\n",
    "            tokens.append(token.text)\n",
    "            ner_tags.append(tag)\n",
    "        lines.append({\"tokens\": (' '.join(tokens)).strip(), \"ner_tags\": (','.join(ner_tags)).strip()})\n",
    "            \n",
    "    # convert to a list of JSON strings\n",
    "    json_lines = [json.dumps(l) for l in lines]\n",
    "\n",
    "    # join lines and save to .json file\n",
    "    json_data = '\\n'.join(json_lines)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(json_data)\n",
    "        \n",
    "    # save to s3    \n",
    "    s3_path = sess.upload_data(path, key_prefix=key_prefix)\n",
    "    print('done! file also saved locally to \"{}\"'.format(filepath))\n",
    "    \n",
    "    return s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-21 00:01:15,488 Reading data from /home/ec2-user/.flair/datasets/ner_english_person\n",
      "2022-05-21 00:01:15,489 Train: /home/ec2-user/.flair/datasets/ner_english_person/bigFile.conll\n",
      "2022-05-21 00:01:15,489 Dev: None\n",
      "2022-05-21 00:01:15,490 Test: None\n",
      "Corpus: 28362 train + 3151 dev + 3501 test sentences\n"
     ]
    }
   ],
   "source": [
    "corpus = get_flair_corpus(dataset_name='NER_ENGLISH_PERSON')\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-21 00:01:21,657 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28362it [00:00, 71885.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-21 00:01:22,054 Dictionary created for label 'ner' with 4 values: M (seen 27887 times), F (seen 4543 times), A (seen 634 times)\n",
      "Dictionary with 4 tags: <unk>, M, F, A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tag_dictionary = corpus.make_label_dictionary(\"ner\")\n",
    "print(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done! file also saved locally to \"train.csv\"\n",
      "done! file also saved locally to \"test.csv\"\n",
      "done! file also saved locally to \"valid.csv\"\n"
     ]
    }
   ],
   "source": [
    "train_input_path = upload_flair_corpus_to_s3(corpus.train, 'train.json')\n",
    "test_input_path = upload_flair_corpus_to_s3(corpus.test, 'test.json')\n",
    "valid_input_path = upload_flair_corpus_to_s3(corpus.dev, 'valid.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test all steps locally first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "https://huggingface.co/docs/datasets/loading_datasets.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_args:\n",
    "    train_file = 'dataset/train.json'\n",
    "    test_file = 'dataset/test.json'\n",
    "    validation_file = 'dataset/valid.json'\n",
    "    text_column_name='tokens'\n",
    "    label_column_name='ner_tags'\n",
    "    task_name='ner'\n",
    "\n",
    "class training_args:\n",
    "    do_train=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-aa398d1e47008087\n",
      "Reusing dataset json (/home/ec2-user/.cache/huggingface/datasets/json/default-aa398d1e47008087/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387bc1bf0eda46f4950beee330985a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {}\n",
    "if data_args.train_file is not None:\n",
    "    data_files[\"train\"] = data_args.train_file\n",
    "if data_args.validation_file is not None:\n",
    "    data_files[\"validation\"] = data_args.validation_file\n",
    "if data_args.test_file is not None:\n",
    "    data_files[\"test\"] = data_args.test_file\n",
    "extension = data_args.train_file.split(\".\")[-1]\n",
    "raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "\n",
    "if training_args.do_train:\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    features = raw_datasets[\"train\"].features\n",
    "else:\n",
    "    column_names = raw_datasets[\"validation\"].column_names\n",
    "    features = raw_datasets[\"validation\"].features\n",
    "\n",
    "if data_args.text_column_name is not None:\n",
    "    text_column_name = data_args.text_column_name\n",
    "elif \"tokens\" in column_names:\n",
    "    text_column_name = \"tokens\"\n",
    "else:\n",
    "    text_column_name = column_names[0]\n",
    "\n",
    "if data_args.label_column_name is not None:\n",
    "    label_column_name = data_args.label_column_name\n",
    "elif f\"{data_args.task_name}_tags\" in column_names:\n",
    "    label_column_name = f\"{data_args.task_name}_tags\"\n",
    "else:\n",
    "    label_column_name = column_names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n",
    "# unique labels.\n",
    "def get_label_list(labels):\n",
    "    unique_labels = set()\n",
    "    for label in labels:\n",
    "        unique_labels = unique_labels | set(label)\n",
    "    label_list = list(unique_labels)\n",
    "    label_list.sort()\n",
    "    return label_list\n",
    "\n",
    "# If the labels are of type ClassLabel, they are already integers and we have the map stored somewhere.\n",
    "# Otherwise, we have to get the list of labels manually.\n",
    "labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)\n",
    "if labels_are_int:\n",
    "    label_list = features[label_column_name].feature.names\n",
    "    label_to_id = {i: i for i in range(len(label_list))}\n",
    "else:\n",
    "    label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
    "    label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LABEL_0': 0, 'LABEL_1': 1}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/ec2-user/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18e6481aafa428199d04773f91679a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-72c4b80aac0f28a2\n",
      "Reusing dataset json (/home/ec2-user/.cache/huggingface/datasets/json/default-72c4b80aac0f28a2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323fbd42af114c92b6aaf1c101932673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset('json',data_files={\n",
    "    'train':'dataset/train.json',\n",
    "    'test':'dataset/test.json',\n",
    "    'valid':'dataset/valid.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': Value(dtype='string', id=None),\n",
       " 'ner_tags': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "# word_ids = inputs.word_ids()\n",
    "# print(labels)\n",
    "# print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = raw_datasets.map(\n",
    "#     tokenize_and_align_labels,\n",
    "#     batched=True,\n",
    "#     remove_columns=raw_datasets[\"train\"].column_names,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Named-Entity-Recognition-on-HuggingFace](https://wandb.ai/biased-ai/Named-Entity%20Recognition%20on%20HuggingFace/reports/Named-Entity-Recognition-on-HuggingFace--Vmlldzo3NTk4NjY)\n",
    "- [`run_ner.py` script in HuggingFace](https://github.com/huggingface/transformers/blob/main/examples/pytorch/token-classification/run_ner.py)\n",
    "- [Named Entity Recognition with Transformers](https://chriskhanhtran.github.io/posts/named-entity-recognition-with-transformers/)\n",
    "- [Token classification in HuggingFace](https://github.com/huggingface/transformers/tree/v4.17.0/examples/pytorch/token-classification)\n",
    "- [Training Custom NER Model Using Flair](https://medium.com/thecyphy/training-custom-ner-model-using-flair-df1f9ea9c762)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_machinelearnear-lightninglite-sagemaker-flair",
   "language": "python",
   "name": "conda_machinelearnear-lightninglite-sagemaker-flair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
